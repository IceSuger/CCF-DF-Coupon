version		feature		preprocess	online_auc
0.23		3,4,8,9,10,12,14,17		
0.24 		3,4,8,9,10,12,14,17,20	MaxAbsScale,OneHot	0.53825059 
0.25		尝试先多项式化，然后把原本的特征3 8 9 14的列号挑出来，也哑编码，最后一起MaxAbsScale   0.57818376 
1.0

-------------
1.2		3,4,8,9,10,12,14,17,20	xgb_no scale no dumm	
0.57312655 
params = {
        "objective": "binary:logistic",
        "booster" : "gbtree",
        "eval_metric": "auc",
        "eta": 0.1,
        "max_depth": 5,
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "silent": 0,
        "seed": 1,
    }
num_boost_round = 20
------------------
v1.3
全都和1.2相同，只是1.3不做poly了。结果线上 0.60074065 
-----------------
v1.4
xgb1 = XGBClassifier(
     learning_rate =0.1,
     n_estimators=500, #好像在eta 0.1时测出来是最好为417
     max_depth=5,
     min_child_weight=1,
     gamma=0,
     subsample=0.8,
     colsample_bytree=0.8,
     objective= 'binary:logistic',
     nthread=4,
     scale_pos_weight=1,
     seed=27)
	 这时候找到的最佳n_estimators = 417.（当时的test-auc:0.998426+小量）
	 
尝试参数xgb2 = XGBClassifier(
     learning_rate =0.05,
     n_estimators=1000, #好像在eta 0.1时测出来是最好为417
     max_depth=7,
     min_child_weight=1,
     gamma=0,
     subsample=0.8,
     colsample_bytree=0.8,
     objective= 'binary:logistic',
     nthread=4,
     scale_pos_weight=1,
     seed=27)
	 这时候找到的最佳n_estimators = 434。（当时test-auc:0.998435+2.08864e-5）
但用第二套参数预测，提交之后，线上成绩只有0.58551380 
---------
v1.5
1. 首先尝试仿照7月集合和全部训练集的关系，构造六月验证集。因为7月的全部userid几乎都在训练集中出现过，那么对六月的验证集，算平均auc时就只考虑userid在X_nojun这一训练集中出现过的条目。
结果是 还他妈不如之前的呢。本来0.494的，现在0.489了，线上可是0.58551380啊。
------------
v1.6
params = {
        "objective": "binary:logistic",
        "booster" : "gbtree",
        "eval_metric": "auc",
        "eta": 0.05,
        "max_depth": 5,
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "silent": 0,
        "nthread":4,
        "seed": 27,
    }
num_boost_round = 100

---------
v1.7 这个版本生成的预测结果，用了和1.6一样的参数。而特征新加了15 16 23 24 25.
线上0.62250179 
---------
v1.8 
params = {
        "objective": "binary:logistic",
        "booster" : "gbtree",
        "eval_metric": "auc",
        "eta": 0.05,
        "max_depth": 5,
        "subsample": 0.8,
        "colsample_bytree": 0.8,
        "silent": 0,
        "nthread":4,
        "seed": 27,
    }
num_boost_round = 130
特征和1.7一样，只是改了num_boost_round，现在只能通过提交看线上成绩来调参了....线下调参没有什么好的办法
0.61848929 
-------
v1.9
尝试n=90	线上得分：
0.62230576 
--------
v1.10
这个版本打算尝试加入特征13，看看效果如何
用n100,n90分别生成两个。结果分别是：n100为0.61917062 ，n90为0.61958875 
特征13有毒啊！？
-----------
v1.11
去掉特征13，尝试了n=96
线上0.62252029 

尝试了改树最大深度，从5改为4，其他参数没变
线上0.61786008
线下Direct Mean auc is:  0.49450479275
加上用户id的限制后，direct：0.489557371452
-----------
v1.12
尝试了改树最大深度，从4改为6，其他参数没变
线上0.62197084 
线下Direct Mean auc is:  0.494429170496
加上用户id的限制后，direct：0.489466482791
------------
v1.13
发现了一个大问题：用xgb是在把处理特征的过程独立成pre_preprocess.py之后。而在这个处理特征的过程中，把训练集里的字段2的空值都填上了0.但是在bigbird中没有对字段11的标记规则进行修改，仍然是对字段2和6同时非空的都标记为1.这直接导致，后来的正例只比反例少一点，二者远不像之前那么不平衡了。当然这只是统计上的规律，关键的是，这他妈的标记是错的啊兄弟！
现在把bigbird里相应的标记规则改成了6非空，2大于0.
那么我们的xgb就要重新面对训练类别不平衡的问题了。
在这个版本中没有对这件事做处理，先用和上个版本一样的参数跑一边看看效果吧。
线上：0.54426046 

等出来结果之后，有如下一些考虑：
1.如果结果有提升，那么根据f_score，尝试去掉importances中占后5%的特征，再训练一个提交；
2.如果结果下降了，那么可能是类别不平衡引起的。那么可以看看特征重要性占的比重，对比一下标错11字段时的特征重要性比重。看看有没有什么发现。
3.无论结果提升还是下降，都可以考虑加上类别平衡，并且尝试不同的weight：0.043或0.1？

另外，根据刚才在网上看的时候的想法。会不会因为特征1的存在，导致了许多训练集中没有的商户的结果被预测错？这一点在1字段有较高重要性的时候尤其有可能啊。
那么：
1. 先重新统计一下，看看测试集合训练集里1字段的交叉率有多少？
2. 尝试去掉特征1，重新训练，提交结果。
3. 注意一点。有好几个统计量特征，也是基于对字段1来统计出来的，也就是很多反应商户特点的特征，在商户不在训练集中时，肯定是瞎jb预测的啊。
--1. OK，看过了。交叉的1558个，测试集里一共1559个。去他妈的。全猜错了。
------------------
v1.14
用0.043的weight做了平衡，线上0.54441005 。比起没平衡的，只是有一丁点提高。
---------
v1.15
尝试了LR，用错误标记训练，结果也不理想：0.54159526 
文件夹中现在那个v1.15_balanced.csv，是用xgb,在用正负类别比例做weight平衡训练集之后得出的结果。我猜它和0.043的不会有多大差别，就不浪费这次提交机会了。这次改为提交v1.16的了。过了12点再提交这个1.15的。
---------
v1.16
0.重新按错误标记进行训练
1.对类别做了平衡
2.根据错误标记下训练出来的xgb给出的fscore，选取特征1,3,8,9,10,12,14,15,16,17,20,23,24,25
线上：0.61855547 
-------------
v1.17
尝试更改预处理策略，不再填充空值，尤其是特征4
